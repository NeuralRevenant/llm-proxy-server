# llm-proxy-server
This acts as a proxy server for making API calls to several LLMs without manually configuring each model in code but using this to route requests to any model as needed.
